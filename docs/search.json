[
  {
    "objectID": "Project_1.html",
    "href": "Project_1.html",
    "title": "Project 1",
    "section": "",
    "text": "Hospitals get rated based on how well they take care of their patients, and these ratings can impact their reputation and even how much funding they receive. People also look at these ratings when deciding which hospital to go to. By analyzing this data, we can figure out why some hospitals do better than others.\nIn this project, I’m using hospital survey data (HCAHPS) to explore how hospitals are rated, what factors might affect those ratings, and how ratings differ from state to state. The main goal is to understand what makes hospitals perform well and maybe even predict their ratings based on what patients say."
  },
  {
    "objectID": "Project_1.html#motivation-and-context",
    "href": "Project_1.html#motivation-and-context",
    "title": "Project 1",
    "section": "",
    "text": "Hospitals get rated based on how well they take care of their patients, and these ratings can impact their reputation and even how much funding they receive. People also look at these ratings when deciding which hospital to go to. By analyzing this data, we can figure out why some hospitals do better than others.\nIn this project, I’m using hospital survey data (HCAHPS) to explore how hospitals are rated, what factors might affect those ratings, and how ratings differ from state to state. The main goal is to understand what makes hospitals perform well and maybe even predict their ratings based on what patients say."
  },
  {
    "objectID": "Project_1.html#main-objective",
    "href": "Project_1.html#main-objective",
    "title": "Project 1",
    "section": "Main Objective",
    "text": "Main Objective\nThis project looks at how satisfied patients are with hospitals in the U.S. using data from the HCAHPS survey. It compares hospital star ratings across different places and checks how the number of survey responses and response rates might affect those ratings. In the end, I build a model to try and predict hospital star ratings based on key parts of the survey."
  },
  {
    "objectID": "Project_1.html#packages-used-in-this-analysis",
    "href": "Project_1.html#packages-used-in-this-analysis",
    "title": "Project 1",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nhere::i_am(\"Project_1.qmd\")\n\nhere() starts at /Users/krystalmaxey/Documents/Math 237/project1\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rsample)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(broom)\nlibrary(yardstick)\n\n\nAttaching package: 'yardstick'\n\nThe following object is masked from 'package:readr':\n\n    spec\n\nlibrary(probably)\n\n\nAttaching package: 'probably'\n\nThe following objects are masked from 'package:base':\n\n    as.factor, as.ordered\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ dials        1.4.0     ✔ recipes      1.3.0\n✔ infer        1.0.7     ✔ tune         1.3.0\n✔ modeldata    1.4.0     ✔ workflows    1.2.0\n✔ parsnip      1.3.1     ✔ workflowsets 1.1.0\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nhospital &lt;- read_csv(here::here(\"Data-1/HCAHPS-Hospital.csv\"), na = c(\"Not Applicable\", \"Not Available\"))\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 443517 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): Facility ID, Facility Name, Address, City/Town, State, ZIP Code, C...\ndbl  (6): Patient Survey Star Rating, Patient Survey Star Rating Footnote, H...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nPackages Used\nlibrary(tidyverse): A collection of R packages for data manipulation, visualization, and functional programming using a consistent grammar and syntax.\nlibrary(rsample): Provides tools for creating resampling objects (like training/testing splits) to support modeling workflows.\nlibrary(janitor): Simplifies data cleaning tasks such as renaming columns, removing empty rows/columns, and formatting messy datasets.\nlibrary(broom): Converts statistical analysis objects into tidy tibbles for easy manipulation and visualization.\nlibrary(yardstick): Provides functions to calculate and compare model performance metrics like accuracy, precision, and recall.\nlibrary(probably): Enhances classification modeling by helping with post-processing of class probabilities, like threshold tuning.\nlibrary(tidymodels): A framework for building, tuning, and evaluating machine learning models using a unified and tidy syntax."
  },
  {
    "objectID": "Project_1.html#design-and-data-collection",
    "href": "Project_1.html#design-and-data-collection",
    "title": "Project 1",
    "section": "Design and Data Collection",
    "text": "Design and Data Collection\nThis code is cleaning up the hospital data to make it easier to work with. First, it fixes the column names so they’re all in a simple format. Then it makes a new version of the data by removing extra info like the hospital’s address and phone number that isn’t needed for the analysis.\n\nhospital &lt;- hospital |&gt;\n  clean_names()\n\nhospital_1 &lt;- hospital |&gt;\n  select(\n    !c(\"address\", \"city_town\",\"telephone_number\", \"hcahps_answer_description\") # Not Finished\n  )\n\nThis code splits the hospital data into three separate datasets so it’s easier to analyze different parts. One dataset keeps only the star ratings, another keeps the answer percentages, and the third keeps the linear mean values. Each one also removes rows where the main value (like the star rating or answer percent) is missing.\n\nhospital_star_rating &lt;- hospital_1 |&gt;\n  select(\n    !c(\"hcahps_answer_percent\", \"hcahps_answer_percent_footnote\", \"hcahps_linear_mean_value\" )\n  ) |&gt;\n  filter(\n    !is.na(`patient_survey_star_rating`)\n    )\n\n\nhospital_ans_perc &lt;- hospital_1 |&gt;\n  select(\n    !c(\"patient_survey_star_rating\", \"patient_survey_star_rating_footnote\", \"hcahps_linear_mean_value\" )\n  ) |&gt;\n  filter(\n    !is.na(`hcahps_answer_percent`)\n    )\n\n\nhospital_linear_mean &lt;- hospital_1 |&gt;\n  select(\n    !c(\"hcahps_answer_percent\", \"hcahps_answer_percent_footnote\", \"patient_survey_star_rating\", \"patient_survey_star_rating_footnote\")\n  ) |&gt;\n  filter(\n    !is.na(`hcahps_linear_mean_value`)\n    )\n\nNext, we changed the shape of the data so that each HCAHPS measure has its own column instead of being stacked in rows. This makes it easier to compare different survey results for each hospital using their facility ID.\n\nhospital_star_rating_1 &lt;- hospital_star_rating |&gt;\n  pivot_wider(id_cols = `facility_id`,\n              names_from = `hcahps_measure_id`,\n              values_from = `patient_survey_star_rating`)\n\nhospital_ans_perc_1 &lt;- hospital_ans_perc |&gt;\n  pivot_wider(id_cols = `facility_id`,\n              names_from = `hcahps_measure_id`,\n              values_from = `hcahps_answer_percent`)\n\nhospital_linear_mean_1 &lt;- hospital_linear_mean |&gt;\n  pivot_wider(id_cols = `facility_id`,\n              names_from = `hcahps_measure_id`,\n              values_from = `hcahps_linear_mean_value`)\n\nFinally, this step puts all the reshaped datasets together into one big table using the facility ID. It also removes any rows with missing info so the data is clean and ready to analyze.\n\nhospital_full &lt;- hospital_star_rating_1 |&gt;\n  inner_join(hospital_ans_perc_1, by = \"facility_id\") |&gt;\n  inner_join(hospital_linear_mean_1, by = \"facility_id\")\n\nhospital_full_clean &lt;- hospital_full |&gt;\n  drop_na()"
  },
  {
    "objectID": "Project_1.html#exploratory-data-analysis",
    "href": "Project_1.html#exploratory-data-analysis",
    "title": "Project 1",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nhospital_star_rating |&gt;\n  filter(!is.na(patient_survey_star_rating)) |&gt;\n  ggplot(aes(x = patient_survey_star_rating)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Patient Survey Star Ratings\", x = \"Star Rating\", y = \"Count\")\n\n\n\n\n\n\n\n\nFigure 1: This histogram shows how hospital star ratings from patient surveys are spread out. The way the ratings are grouped can give us insight into how patients generally feel or if there’s a pattern in how ratings are given.\nFinding: Most hospitals have around a 3-star rating, which suggests average patient satisfaction. Only a few hospitals get very low (1-star) or very high (5-star) ratings, meaning there isn’t a lot of extreme feedback either way.\n\nggplot(hospital_full_clean, aes(x = H_STAR_RATING)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Distribution of Hospital Star Ratings\",\n       x = \"Star Rating\", y = \"Count\")\n\n\n\n\n\n\n\n\nFigure 2: This plot shows how hospital star ratings are spread across different facilities. It helps us see if the ratings tend to be higher, lower, or spread out evenly.\nFinding: Just like the patient survey ratings, overall hospital ratings mostly fall in the middle range. This suggests that most hospitals provide similar levels of care, with fewer hospitals standing out as either exceptionally good or poor.\n\nhospital |&gt;\n  filter(!is.na(patient_survey_star_rating)) |&gt;\n  group_by(state) |&gt;\n  summarise(avg_rating = mean(patient_survey_star_rating, na.rm = TRUE)) |&gt;\n  slice_max(avg_rating, n = 5) |&gt;\n  ggplot(aes(x = reorder(state, avg_rating), y = avg_rating)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 5 States by Average Patient Survey Star Rating\",\n       x = \"State\", y = \"Average Star Rating\")\n\n\n\n\n\n\n\n\nFigure 3: This chart ranks states based on their average patient survey star ratings, helping us spot regional differences in patient satisfaction.\nFinding: There’s a noticeable variation in average ratings from state to state. Some states consistently have higher ratings, which could point to differences in hospital care quality or varying patient expectations across regions.\n\n# Get top 5 states by average patient survey star rating\ntop_states &lt;- hospital |&gt;\n  filter(!is.na(patient_survey_star_rating)) |&gt;\n  group_by(state) |&gt;\n  summarise(avg_rating = mean(patient_survey_star_rating, na.rm = TRUE)) |&gt;\n  slice_max(avg_rating, n = 5) |&gt;\n  pull(state)  # extract vector of state names\n\n# Plot only those states\nhospital_full_clean |&gt;\n  left_join(hospital, by = \"facility_id\") |&gt;\n  filter(state %in% top_states) |&gt;\n  ggplot(aes(x = state, y = H_STAR_RATING)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(title = \"Star Ratings for Top 5 States by Patient Survey Rating\",\n       y = \"Star Rating\", x = \"State\")\n\n\n\n\n\n\n\n\nFigure 4: This boxplot shows how star ratings vary across states, highlighting those with consistent ratings and others with more extreme differences.\nFinding: Some states have ratings that are closely grouped together, meaning hospitals there tend to perform similarly. Other states have more spread-out ratings, suggesting there are bigger differences in care quality within those states.\n\nhospital |&gt;\n  filter(!is.na(patient_survey_star_rating), !is.na(number_of_completed_surveys)) |&gt;\n  ggplot(aes(x = number_of_completed_surveys, y = patient_survey_star_rating)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Star Rating vs Number of Completed Surveys\",\n       x = \"Completed Surveys\", y = \"Star Rating\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFigure 5: This visualization looks at whether hospitals with more survey responses have different ratings, which could show if the number of responses affects the reliability of the ratings.\nFinding: There’s no clear connection between the number of completed surveys and the star rating. This indicates that the rating is not significantly influenced by how many people respond to the survey.\n\nhospital_full_clean |&gt;\n  left_join(hospital |&gt; select(facility_id, survey_response_rate_percent), \n            by = \"facility_id\") |&gt;\n  ggplot(aes(x = survey_response_rate_percent, y = H_STAR_RATING)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Survey Response Rate vs Star Rating\",\n       x = \"Survey Response Rate (%)\", y = \"Star Rating\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFigure 6: This plot examines whether higher response rates are linked to better ratings, which could indicate more engaged patients or hospitals with better practices.\nFinding: There’s a slight positive trend, suggesting that hospitals with higher response rates tend to have slightly better ratings. This could be due to better patient engagement or more transparent hospital practices."
  },
  {
    "objectID": "Project_1.html#modeling",
    "href": "Project_1.html#modeling",
    "title": "Project 1",
    "section": "Modeling",
    "text": "Modeling\nIn this step, we’re preparing the data and creating a model to predict hospital star ratings.\n\nSelecting data: We choose the important columns related to hospital performance (like cleanliness and recommendation scores) and remove any rows with missing information.\nSplitting the data: We split the data into two parts: 80% for training the model and 20% for testing how well the model works.\nBuilding the model: We create a linear regression model to predict hospital star ratings based on the other scores. We train the model using the training data.\nThis helps us predict hospital star ratings using the selected data.\n\n\nmodel_data &lt;- hospital_full_clean |&gt;\n  select(H_STAR_RATING,\n         H_HSP_RATING_LINEAR_SCORE,\n         H_RECMND_LINEAR_SCORE,\n         H_CLEAN_LINEAR_SCORE,\n         H_COMP_1_LINEAR_SCORE,\n         H_COMP_2_LINEAR_SCORE,\n         H_COMP_3_LINEAR_SCORE) |&gt;\n  drop_na()\n\n\nset.seed(123)\ndata_split &lt;- initial_split(model_data, prop = 0.8)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(H_STAR_RATING ~ ., data = train_data)\n\n\ntidy(lm_model) |&gt;\n  arrange(desc(abs(estimate)))\n\n# A tibble: 7 × 5\n  term                      estimate std.error statistic  p.value\n  &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               -20.7      0.346      -59.8  0       \n2 H_COMP_1_LINEAR_SCORE       0.0817   0.00694     11.8  3.37e-31\n3 H_COMP_2_LINEAR_SCORE       0.0751   0.00480     15.6  9.96e-53\n4 H_COMP_3_LINEAR_SCORE       0.0398   0.00299     13.3  4.25e-39\n5 H_HSP_RATING_LINEAR_SCORE   0.0319   0.00814      3.92 8.96e- 5\n6 H_RECMND_LINEAR_SCORE       0.0212   0.00503      4.23 2.45e- 5\n7 H_CLEAN_LINEAR_SCORE        0.0207   0.00208      9.93 8.07e-23\n\n\n\nHow to Interpret\n\nestimate: This is the effect size. For example, an estimate of 0.02 for H_RECMND_LINEAR_SCORE means that for each 1 point increase in that score, the predicted star rating increases by 0.02.\np-value: If it is below 0.05, that predictor is statistically significant.\nHighest absolute estimate values = strongest impact on the prediction.\n\n\ncoef_plot_data &lt;- tidy(lm_model) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(term = str_replace(term, \"_LINEAR_SCORE\", \"\"), \n         term = str_replace(term, \"H_\", \"\"),              \n         term = str_replace(term, \"COMP_\", \"COMP \"),       \n         term = fct_reorder(term, abs(estimate)))\n\nggplot(coef_plot_data, aes(x = term, y = estimate, fill = estimate &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Predictor Effects on Hospital Star Rating\",\n       x = \"Predictor\",\n       y = \"Coefficient Estimate\") +\n  scale_fill_manual(values = c(\"steelblue\", \"steelblue\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 7: Effects of Patient Survey Measures on Hospital Star Ratings. This shows the coefficients from a linear regression model predicting hospital star ratings based on patient survey scores. The bars represent how much the star rating is expected to change with a one-unit increase in each survey measure. All variables shown are statistically significant (p &lt; 0.001).\nFinding: The analysis reveals that certain patient survey measures have a strong impact on hospital star ratings. Specifically, improvements in survey measures like patient communication, cleanliness, and responsiveness are associated with higher star ratings. The significant relationship between these factors indicates that hospitals focusing on these areas may see improved ratings.\nThe purpose of this next chunk of code is to build and evaluate a linear regression model that predicts a hospital’s star rating (H_STAR_RATING) based on several performance scores, such as cleanliness and patient recommendation scores. The code selects relevant data, splits it into training and testing sets, trains the model using the training data, makes predictions on the test data, and then measures how accurate the predictions are. Finally, it creates a graph to compare the predicted ratings with the actual ratings.\n\nmodel_data &lt;- hospital_full_clean |&gt;\n  select(H_STAR_RATING,\n         H_HSP_RATING_LINEAR_SCORE,\n         H_RECMND_LINEAR_SCORE,\n         H_CLEAN_LINEAR_SCORE,\n         H_COMP_1_LINEAR_SCORE,\n         H_COMP_2_LINEAR_SCORE,\n         H_COMP_3_LINEAR_SCORE) |&gt;\n  drop_na()\n\n\nset.seed(123)\ndata_split &lt;- initial_split(model_data, prop = 0.8)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(H_STAR_RATING ~ ., data = train_data)\n\n\npredict_test &lt;- predict(lm_model, new_data = test_data) |&gt;\n  bind_cols(test_data)\n\n\nmetrics(predict_test, truth = H_STAR_RATING, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.340\n2 rsq     standard       0.864\n3 mae     standard       0.277\n\nggplot(predict_test, aes(x = H_STAR_RATING, y = .pred)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  labs(title = \"Predicted vs Actual Hospital Star Ratings\",\n       x = \"Actual Rating\", y = \"Predicted Rating\")\n\n\n\n\n\n\n\n\nFigure 8: This comparison evaluates how well the model predicts star ratings using HCAHPS linear scores. Points close to the diagonal line indicate better predictions.\nFinding: The model predicts star ratings with moderate accuracy. While the predicted ratings are generally close to the actual ratings, there are some deviations, suggesting that while linear scores are helpful, they don’t account for all factors that influence hospital ratings."
  },
  {
    "objectID": "Project_1.html#conclusion",
    "href": "Project_1.html#conclusion",
    "title": "Project 1",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis looks at hospital performance using HCAHPS survey data and reveals some interesting trends. Most hospitals have average patient satisfaction ratings, with many receiving around 3 stars. This suggests that while hospitals generally provide decent care, very few stand out with either very high or very low ratings. Hospital star ratings follow a similar pattern, indicating that the level of care is relatively consistent across different hospitals.\nThe analysis also highlights that hospitals in certain states perform better than others, showing that the quality of care can differ by region. These findings are important because they emphasize the need to understand both patient satisfaction and the factors influencing hospital ratings, such as response rates and the number of surveys completed. Moving forward, it could be valuable to develop models that predict a hospital’s rating based on survey responses, offering a clearer picture of how hospitals are performing."
  }
]